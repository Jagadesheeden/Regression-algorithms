# -*- coding: utf-8 -*-
"""Multiple_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zp2zbg6TVbSVr9gm9fWhcTRUlUMoorN6

Multiple regression is a statistical technique that can be used to analyze the relationship between a single dependent variable and several independent variables.

It is the same as the previous algorithm(linear regression) but this regression has many features

We will use the formula y = m0x0+m1x1+m2x2+m3x3+m4x4+c
where, x1,x2,x3,x4 are the coefficient, 
       c is the interpret, 
       m0,m1,m2,m3 is the features, 
       y is the target.

We will follow the same step as we did in the previous algorithm.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

startup_df = pd.read_csv('/content/50_Startups.csv')

startup_df.head()

startup_df.dtypes

"""The state value are in object, we need to convert it into float.

We will we use dummies for changing the object value to float.

yes_no is a series value contains Y and N
"""

yes_no = pd.Series(['Y','N','Y','N','Y','N','Y','N'])

yes_no

"""If we change the Y and Y to dummies we are getting a 0's and 1's column, the Y and N values are changed to 0 and 1.

If the value is Y we will get 1 on Y and 0 on N, If the value is N we will get 1 on N and 0 on Y.
"""

pd.get_dummies(yes_no)

startup_df.head()

"""Applying the same dummies value for state column and we are adding drop_first as true it will delete a column in dummies.

Storing the value as state_data
"""

state_data = pd.get_dummies(startup_df['State'],drop_first = True)

state_data.head()

"""To include this dummies value we need to remove the state column in the startup_df. TO include that use drop and give axix = 1.

If the axis is 1 it denotes column, If the axix is 0 it denotes row.
"""

startup_df = startup_df.drop(axis = 1,labels = 'State')

startup_df.head()

"""Add the value in the startup_df"""

state_data.append(startup_df)

startup_data = pd.concat([state_data,startup_df],axis = 1)

startup_data.head()

"""We will take features values from column 1 to 4"""

x = np.array(startup_data.iloc[:,0:5])

x

"""Target values we will take from last column."""

y = np.array(startup_data.iloc[:,-1])

y

print(x.shape)
print(y.shape)

"""Reshape it into 2D """

y = y.reshape((-1,1))

print(x.shape)
print(y.shape)

"""Split the data for training and testing"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 2022)

print("Shape of Training Input",x_train.shape)
print("Shape of Training Output",y_train.shape)

print("Shape of Testing Input",x_test.shape)
print("Shape of Testing Output",y_test.shape)

"""Import the regression model and fit the data into the regression model"""

from sklearn.linear_model import LinearRegression

mlr = LinearRegression()

mlr.fit(x_train,y_train)

y_pred = mlr.predict(x_test)

for i in range(0,len(y_pred)):
    print("Actual: ",y_test[i]," and Predict: ",y_pred[i])

"""It gives the R squared value as 94 percentage but we cannot use rsquared value for multiple linear regression we will get a wrong accuracy.

For getting a perfect accuracy we need to use adj rsquared value.
"""

mlr.score(x,y)

"""Importing the other statsmodel for performing adj rsquare but it will not add interpret by it self we need to give give interpret as feature."""

import statsmodels.formula.api as smf

x

"""Adding a extra column for interpret"""

x_for_statsmodel = np.append(arr = np.ones((50,1)).astype(int),values = x,axis = 1)

"""Importing regression model from OLS and fit our data within it, the summary will give the adj rsquared value

OLS - Ordinary Least Square method
"""

from statsmodels.regression.linear_model import OLS

model = OLS(endog = y,exog = x_for_statsmodel)

model = model.fit()

"""We can see the adj rsquared here, which is lesser than rsquared value to improve the adj rsquared value we can remove the overfitting column."""

print(model.summary())

"""Removed the third column, it has the highest P value"""

x_for_statsmodel = x_for_statsmodel[:,[0,1,3,4,5]]
model = OLS(endog = y,exog = x_for_statsmodel)
model = model.fit()
print(model.summary())

"""Removed the second column, it has the highest P value"""

x_for_statsmodel = x_for_statsmodel[:,[0,2,3,4]]
model = OLS(endog = y,exog = x_for_statsmodel)
model = model.fit()
print(model.summary())

"""Removed the third column, it has the highest P value"""

x_for_statsmodel = x_for_statsmodel[:,[0,1,3]]
model = OLS(endog = y,exog = x_for_statsmodel)
model = model.fit()
print(model.summary())

"""After removing another column the adj rsquared the value of adj rquared value is reduced it is called underfitting data, we better stop it in the previous step"""

x_for_statsmodel = x_for_statsmodel[:,[0,1]]
model = OLS(endog = y,exog = x_for_statsmodel)
model = model.fit()
print(model.summary())