# -*- coding: utf-8 -*-
"""Support_Vector_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RzV83rjkmNO8vRkLSDiKOPTYQuQ0PDag

https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC

SVM (Support Vector Machines) - Classification

SVR (Support Vector Regression) - Regression

Effective in high dimensional spaces.

Support Vectors lie next to the closest value of a class and decision boundary line is created between these support vectors to be as distanced as possible.

Hyperplane / Decision Boundary separates the values of two classes helped from Support Vectors

Kernel - Math Fun. that converts input data to a certain format. This new format is used for finding hyperplane. (Linear, Poly, RBF & Sigmoid)

C: Regularization parameter (Penalty)

Constraints are the unwanted value which present in otherside of the graph if you give large contraints it will ignore some contraints if the c value is small it will try harder to get those values 

Large C Value - Ignores certain constraints

Small C Value - Tries to consider harder constraints

Gamma - Only with Poly, Sigmoid, RBF (Helps calculate curvature of model).

Low Gamma - Low Curve

High Gamma - High Curve

C and gamma ranges from 0 to 100

C & Gamma: 0.001, 0.01, 0.1, 1, 10, 100

Manually: Tough Task Easier to figure out which param to use (Gamma, C, Kernel): Use GridSearch

Proceed all the steps till scaling
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import pandas as pd
temperature_data = pd.read_csv('https://data.giss.nasa.gov/gistemp/graphs/graph_data/Global_Mean_Estimates_based_on_Land_and_Ocean_Data/graph.txt',
            sep='    ',
            skiprows = 5,
            names = ['Year','No_Smoothing_Temp','Smoothing_Temp'])

temperature_data.head()

x = temperature_data.loc[:,'Year'].values

y = temperature_data.loc[:,'No_Smoothing_Temp'].values

x.shape

x=x.reshape(-1,1)
y=y.reshape(-1,1)

"""Feature Scaling

Normalization or Standardization to ensure that all data are in the same scaled format.

If we have huge difference in value it will not consider the value so we need
to change to scaled format.

Normalization: Values between [0,1]

Standardization: Mean of all values should be 0. Also, standard deviation should be 1.

https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standard#sklearn.preprocessing.StandardScaler

https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer

Whether to Standardize or to Normalize?

Standardize: Gaussian Distribution

Normalize: Non Gaussian Distribution

If you want outliers to have low impact in your model, use standardization. Else, use normalization.

Import standardscaler from sklearn and fit x and y value
"""

from sklearn.preprocessing import StandardScaler
scale_x = StandardScaler()
x = scale_x.fit_transform(x)

x[:,:]

scale_y = StandardScaler()
y = scale_y.fit_transform(y)

y[:,:]

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 2022)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""Import SVR from sklearn"""

from sklearn.svm import SVR

svr_regressor = SVR(kernel = 'rbf',C = 100,gamma=100)

svr_regressor.fit(x_train,y_train)

svr_regressor.score(x,y)

y_pred = svr_regressor.predict(x_test)

from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test,y_pred)

"""Before giving input we need to unscale it by inverse transform method"""

scale_y.inverse_transform([[mean_absolute_error(y_test,y_pred)]])

plt.plot(x,y)
plt.plot(x,svr_regressor.predict(x))

x[:5,:]

input = 2022
print("UnScaled Input: ",input)
input_scaled = scale_x.transform([[input]])
print("Scaled Input: ",input_scaled)
output_scaled = svr_regressor.predict(input_scaled)
print("Scaled Output: ",output_scaled)
output_unscaled = scale_y.inverse_transform([output_scaled])
print("Unscaled Output: ",output_unscaled)

"""I have given various values for c and gamma to show the difference 

We can see a different curve for different c and gamma
"""

svr_regressor = SVR(kernel = 'rbf',C = 10,gamma=10)
svr_regressor.fit(x_train,y_train)
plt.plot(x,y)
plt.plot(x,svr_regressor.predict(x))

svr_regressor = SVR(kernel = 'rbf',C = 20,gamma=20)
svr_regressor.fit(x_train,y_train)
plt.plot(x,y)
plt.plot(x,svr_regressor.predict(x))

svr_regressor = SVR(kernel = 'rbf',C = 30,gamma=30)
svr_regressor.fit(x_train,y_train)
plt.plot(x,y)
plt.plot(x,svr_regressor.predict(x))

svr_regressor = SVR(kernel = 'rbf',C = 60,gamma=60)
svr_regressor.fit(x_train,y_train)
plt.plot(x,y)
plt.plot(x,svr_regressor.predict(x))

svr_regressor = SVR(kernel = 'rbf',C = 80,gamma=80)
svr_regressor.fit(x_train,y_train)
plt.plot(x,y)
plt.plot(x,svr_regressor.predict(x))

svr_regressor = SVR(kernel = 'rbf',C = 0.1,gamma=0.1)
svr_regressor.fit(x_train,y_train)
plt.plot(x,y)
plt.plot(x,svr_regressor.predict(x))

svr_regressor = SVR(kernel = 'rbf',C = 0.1,gamma=100)
svr_regressor.fit(x_train,y_train)
plt.plot(x,y)
plt.plot(x,svr_regressor.predict(x))

svr_regressor = SVR(kernel = 'rbf',C = 100,gamma=0.1)
svr_regressor.fit(x_train,y_train)
plt.plot(x,y)
plt.plot(x,svr_regressor.predict(x))