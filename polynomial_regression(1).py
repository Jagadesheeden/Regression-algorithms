# -*- coding: utf-8 -*-
"""Polynomial_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xHJRZh5U5nkUJRxD4uqlsPHbk1GXk4BL

Polynomial Regression is a regression algorithm that models the relationship between a dependent(y) and independent variable(x) as nth degree polynomial

y= b0+b1x1+ b2x12+ b2x13+...... bnx1n

It is also called the special case of Multiple Linear Regression in ML. Because we add some polynomial terms to the Multiple Linear regression equation to convert it into Polynomial Regression.
It is a linear model with some modification in order to increase the accuracy.
The dataset used in Polynomial regression for training is of non-linear nature.
It makes use of a linear regression model to fit the complicated and non-linear functions and datasets.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""lets see a demo for polynomial regression without using a actual data

Given x range from 1 to 40 and y is the x to the power of 4
"""

x = np.array(range(1,41))

y = x**4

y

"""Giving a lower value for y[5] and y[-2]"""

y[5] = 23
y[-2] = 13441

"""Usually polynomial regression will give a curve and the break in that curve is due to the change we made with y"""

plt.plot(x,y)

"""Reshaping and fitting our data into our as we did before after the usual step we will use polynomial features"""

x = x.reshape((-1,1))
y = y.reshape((-1,1))

from sklearn.model_selection import  train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.linear_model import LinearRegression
linear_reg = LinearRegression()
linear_reg.fit(x_train,y_train)

y_pred = linear_reg.predict(x_test)

"""When we compare the predicted output and real value we can see the huge difference in both the model, our model is so bad we need to improve on that

Increasing in degrees too much will give you overfit solution.

Decreasing in degrees too much (Linear) will give you underfit solution

"""

plt.plot(x,linear_reg.predict(x))
plt.plot(x,y)

"""Importing polynomial features from sklearn"""

from sklearn.preprocessing import PolynomialFeatures

"""Storing our model as poly_feat and giving the degree 2"""

poly_feat = PolynomialFeatures(degree = 2)

"""Usually we use fit for fitting our data into our model now we use fit_transform(x) because we are transforming the x degree to 2"""

x_degree_two = poly_feat.fit_transform(x)

print(x)

"""After transform the data looks like this first column is the constant, second row is degree one and third row is degree two"""

print(x_degree_two)

"""Fitting the polynomail value of X and value of y to our linearRegression model

When we compare the three different chart we can see the difference with three different values, the polynomial feature(orange line) fitting good in our model
"""

poly_model = LinearRegression()
poly_model.fit(x_degree_two,y)
plt.plot(x,y)
plt.plot(x,poly_model.predict(x_degree_two))
plt.plot(x,linear_reg.predict(x))

"""When we give dergree 6 the model is overfitting """

poly_feat = PolynomialFeatures(degree = 6)
x_degree = poly_feat.fit_transform(x)
poly_model = LinearRegression()
poly_model.fit(x_degree,y)
plt.plot(x,y)
plt.plot(x,poly_model.predict(x_degree))
poly_model.score(x_degree,y)

x_degree

"""Now we will implement the same with real world data

For data dataset we will import data from that link below
"""

import pandas as pd
temperature_data = pd.read_csv('https://data.giss.nasa.gov/gistemp/graphs/graph_data/Global_Mean_Estimates_based_on_Land_and_Ocean_Data/graph.txt',
            sep='    ',
            skiprows = 5,
            names = ['Year','No_Smoothing_Temp','Smoothing_Temp'])

temperature_data.head()

x = temperature_data.loc[:,'Year'].values
y = temperature_data.loc[:,'No_Smoothing_Temp'].values

x.shape

y.shape

y = y.reshape((-1,1))

x = x.reshape((-1,1))

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

import matplotlib.pyplot as plt 
plt.plot(x,y)

from sklearn.linear_model import LinearRegression
linear_reg_model = LinearRegression()
linear_reg_model.fit(x_train,y_train)

y_pred = linear_reg_model.predict(x_test)

for i in range(0,len(y_test)):
  print("Actual: ",y_test[i]," Predicted: ",y_pred[i])

"""Mean absolute error is the average error between the values

There are many metrics in sklearn for measuring different errors
"""

from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test,y_pred)

plt.plot(x,linear_reg_model.predict(x))
plt.plot(x,y)

linear_reg_model.predict([[2023]])

from sklearn.preprocessing import PolynomialFeatures
poly_feat = PolynomialFeatures(degree = 4)
x_degree = poly_feat.fit_transform(x_train)

x_degree

poly_model = LinearRegression()
poly_model.fit(x_degree,y_train)

"""The improper curve happen due to improper arrangement of data the first year value is 1993 and second is 1995 but the fourth is 2013 and fifth is 1959 so the curve will move back and front"""

plt.plot(x,y)
plt.plot(x_train,poly_model.predict(x_degree))

x_train[:5,:]

x_degree[:5,:]

"""Zipped will arrange the data in asc order along with the values in that row"""

zipped_values = zip(x_train,poly_model.predict(x_degree))
un = sorted(zipped_values, key = lambda x : x[0])

res = list(zip(*un))

import numpy as np
year_data = np.array(res[0])

year_data[:5,:]

x[:5,:]

poly_output = np.array(res[1])

poly_output[:5,:]

plt.plot(x,y)
plt.plot(year_data,poly_output)

poly_feat.transform([[2022]])

poly_model.predict(poly_feat.transform([[2021]]))